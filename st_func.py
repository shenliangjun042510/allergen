import streamlit as st

import pandas as pd
import os
import time
from Bio import SeqIO
import requests
from bs4 import BeautifulSoup

from cross_allergen import *
from parser import *

def search_allergen_org_with_real_uniprot_st(source_keyword="peanut", fasta_dir="./fasta_files"):
    base_url = "https://www.allergen.org"
    search_url = f"{base_url}/search.php"
    params = {
        "allergenname": "",
        "allergensource": source_keyword,
        "TaxSource": "",
        "TaxOrder": "",
        "foodallerg": "all",
        "bioname": "",
    }

    with st.spinner(f"ğŸ” æ­£åœ¨æœç´¢ {source_keyword} ç›¸å…³è¿‡æ•åŸ..."):
        resp = requests.get(search_url, params=params)
        if resp.status_code != 200:
            st.error(f"âŒ è¯·æ±‚å¤±è´¥: {resp.status_code}")
            return []

    soup = BeautifulSoup(resp.text, "html.parser")
    table = soup.find("table", {"border": "1"})
    if not table:
        st.warning("âš ï¸ æœªæ‰¾åˆ°ç»“æœè¡¨æ ¼")
        return []

    rows = table.find_all("tr")[2:]
    total_allergens = len(rows)
    st.info(f"å…±æ‰¾åˆ° {total_allergens} ä¸ªè¿‡æ•åŸæ¡ç›®")

    os.makedirs(fasta_dir, exist_ok=True)
    results = []

    progress_bar = st.progress(0)  # è¿›åº¦æ¡
    status_text = st.empty()       # åŠ¨æ€æ–‡æœ¬

    missing_ids = []

    for idx, row in enumerate(rows, 1):
        cols = row.find_all("td")
        try:
            allergen_name = cols[1].text.strip()
            biological_name = cols[2].text.strip()
            iuis_id = cols[1].find("a")["href"].split("=")[-1].strip()
            uniprot_id = get_uniprot_id_from_detail_page(iuis_id)
        except:
            uniprot_id = None

        sequence = None
        if uniprot_id:
            fasta_url = f"https://rest.uniprot.org/uniprotkb/{uniprot_id}.fasta"
            fasta_resp = requests.get(fasta_url)
            if fasta_resp.ok:
                sequence = fasta_resp.text
                fasta_path = os.path.join(fasta_dir, f"{uniprot_id}.fasta")
                with open(fasta_path, "w", encoding="utf-8") as f:
                    f.write(sequence)
            else:
                missing_ids.append(uniprot_id)
        else:
            missing_ids.append(allergen_name)

        results.append({
            "allergen_name": allergen_name,
            "biological_name": biological_name,
            "iuis_id": iuis_id,
            "uniprot_id": uniprot_id,
            "sequence": sequence
        })

        progress_bar.progress(idx / total_allergens)
        status_text.text(f"[{idx}/{total_allergens}] å·²è·å–: {allergen_name}")
    if missing_ids:
        st.warning(f"âš ï¸ æœªæ‰¾åˆ° UniProt ID çš„è¿‡æ•åŸ: {', '.join(missing_ids)}")

    return results

def predict_cross_allergen_streamlit(species_name, query_fasta_dir="fasta_files", db_fasta="./naive/uniprotkb_allergen_2025_07_22.fasta", k=5, cpu=None, local_enabled=True):

    # 1. è¯»å–ç¼“å­˜çš„ CSV
    species_csv = f"species_cache/{species_name}_allergens.csv"
    if not os.path.exists(species_csv):
        st.error(f"âŒ æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶: {species_csv}ï¼Œè¯·å…ˆè¿è¡Œç‰©ç§è¿‡æ•åŸæœç´¢ã€‚")
        return

    allergen_df = pd.read_csv(species_csv)
    if "uniprot_id" not in allergen_df.columns:
        st.error("âŒ CSV æ–‡ä»¶ç¼ºå°‘ 'uniprot_id' åˆ—ï¼Œæ— æ³•ç»§ç»­ã€‚")
        return

    allergen_ids = allergen_df["uniprot_id"].tolist()
    st.info(f"ğŸ” ä»ç¼“å­˜è¯»å–åˆ° {len(allergen_ids)} ä¸ª {species_name} è¿‡æ•åŸ ID")

    # 2. åŠ è½½æ•°æ®åº“åºåˆ—
    db_records = list(SeqIO.parse(db_fasta, "fasta"))

    # 3. æ‰¾åˆ°å¯¹åº”çš„ fasta æ–‡ä»¶
    query_files = []
    for uid in allergen_ids:
        fasta_path = os.path.join(query_fasta_dir, f"{uid}.fasta")
        if os.path.exists(fasta_path):
            query_files.append(fasta_path)

    if not query_files:
        st.error("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ query FASTA æ–‡ä»¶")
        return

    st.info(f"ğŸ“‚ æ‰¾åˆ° {len(query_files)} æ¡å¾…æµ‹åºåˆ—")

    # 4. è¿è¡Œæ£€æµ‹
    all_results = []
    total_start = time.time()
    log_box = st.empty()
    log_text = ""

    for idx, qfile in enumerate(query_files, 1):
        query_seq = SeqIO.read(qfile, "fasta")
        log_text += f"\n[{idx}/{len(query_files)}] å¤„ç†åºåˆ—: {query_seq.id} ({qfile})"
        log_box.code(log_text)

        start = time.time()
        results = check_cross_parallel(query_seq, db_records, win=80, id_thresh=0.35, n_jobs=cpu, local_enabled=local_enabled)
        end = time.time()

        if results:
            results = dedup_by_os(results)
            results.sort(key=lambda x: x["score"], reverse=True)
            for r in results:
                r['query_id'] = query_seq.id
            all_results.extend(results)
            log_text += f"\n  âœ”ï¸ æ‰¾åˆ° {len(results)} ä¸ªæ½œåœ¨äº¤å‰è¿‡æ•åŸï¼Œè€—æ—¶ {round(end-start, 2)} s"
        else:
            log_text += "\n  âš ï¸ æœªå‘ç°æ½œåœ¨äº¤å‰è¿‡æ•åŸ"

        log_box.code(log_text)

    total_end = time.time()

    if not all_results:
        st.warning("âŒ å…¨éƒ¨åºåˆ—å‡æœªå‘ç°æ½œåœ¨äº¤å‰è¿‡æ•åŸ")
        return

    # 5. ä¿å­˜ txt
    os.makedirs("result_cache", exist_ok=True)
    result_txt = f"result_cache/{species_name}.txt"
    with open(result_txt, 'w', encoding='utf-8') as f:
        current_query = None
        for i, r in enumerate(all_results, 1):
            if r['query_id'] != current_query:
                current_query = r['query_id']
                f.write(f"\n\n==== Query: {current_query} ====\n")
            f.write(f"{i}. {r['id']}\n"
                    f"   ğŸ“› åç§°: {r['description']}\n"
                    f"   ğŸ§¬ Identity: {r['identity']:.3f}\n"
                    f"   ğŸ”— Has 6-mer match: {r['has_6mer']}\n"
                    f"   ğŸ“Š ç»¼åˆå¾—åˆ†: {r['score']:.3f}\n"
                    f"   ğŸ” åŒ¹é…æ¨¡å¼: {r['match_mode']}\n")

    # 6. è½¬ DataFrame
    content = {key: [] for key in all_results[0].keys()}
    for item in all_results:
        for key in content.keys():
            if key == 'id':
                item[key] = item[key].split('|')[1]
            content[key].append(item[key])
    df = pd.DataFrame(content)

    st.success(f"ğŸ‰ æ‰€æœ‰æŸ¥è¯¢åºåˆ—å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {round(total_end - total_start, 2)} s")

    return df
